# -*- coding: utf-8 -*-
"""Model Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a57QQX_b1Mf3A1aA1xN9nDDOMGnY41Z7
"""

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd

import numpy as np
from sklearn.model_selection import train_test_split

lp1 = pd.read_csv('/content/drive/MyDrive/Hyunmin Yoo/Data/cleaned_lp1.csv')
lp2 = pd.read_csv('/content/drive/MyDrive/Hyunmin Yoo/Data/cleaned_lp2.csv')
lp3 = pd.read_csv('/content/drive/MyDrive/Hyunmin Yoo/Data/cleaned_lp3.csv')
lp4 = pd.read_csv('/content/drive/MyDrive/Hyunmin Yoo/Data/cleaned_lp4.csv')
lp5 = pd.read_csv('/content/drive/MyDrive/Hyunmin Yoo/Data/cleaned_lp5.csv')

dataset = lp1
dataset.head()

X = dataset.drop('class',axis=1)
y = dataset['class']

X.head()

y.head()

len(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#https://scikit-learn.org/stable/modules/multiclass.html

#https://panjeh.medium.com/scikit-learn-hyperparameter-optimization-for-mlpclassifier-4d670413042b

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
mlp = MLPClassifier()

parameter_space = {
    'hidden_layer_sizes':[(20,20,5),(24,23),(22,23),(20,20),(10,15),(3,4,5),(5,21,22),(20,25),(21,22),(25,25,20,30),(13,13,15),(3,4,5,6,7,8,9,10,11,12),(3,4),(20),(3,4,5,6),(10),(5)],
    'activation':['identity', 'logistic','tanh','relu'],
    'solver': ['lbfgs','sgd','adam'],
    'alpha': [0.0001,0.05, 0.001, 0.01, 0.5],
    'learning_rate': ['constant','adaptive','invscaling'],
    'max_iter': [1,100,200,500,1000]
}

gs = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=2)
gs.fit(X_train, y_train)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

rf = RandomForestClassifier(random_state=42)

parameter_space = {
    'n_estimators': [100, 200, 500, 700, 1000], 'max_depth': [None, 10 ,20], 'min_samples_split': [2], 'min_samples_leaf': [1], 'max_features': ['sqrt']
}

rf = GridSearchCV(rf, parameter_space, n_jobs=-1, cv=2)
rf.fit(X_train, y_train)

print('Best parameters found:\n', rf.best_params_)

from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_train)

xgb = XGBClassifier(random_state=42)

parameter_space = {
  'max_depth': [3, 4, 5],
  'min_child_weight': [1, 3, 5],
  'subsample': [0.7, 0.8, 0.9],
  'colsample_bytree': [0.7, 0.8, 0.9],
  'gamma': [0, 0.1, 0.3],
  'learning_rate': [0.05, 0.1, 0.2],
  'n_estimators': [100, 200, 300],
  'reg_alpha': [0, 0.1, 1],
  'reg_lambda': [1, 2, 5]
}

xgb = GridSearchCV(xgb, parameter_space, n_jobs=-1, cv=2)
xgb.fit(X_train, y_encoded)

print('Best parameters found:\n', xgb.best_params_)

print('Best parameters found:\n', log_reg_model.best_params_)

means = log_reg_model.cv_results_['mean_test_score']
stds = log_reg_model.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, log_reg_model.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))

#log_reg_model = MLPClassifier(hidden_layer_sizes=(22,23), activation='logistic', alpha = 0.0001, learning_rate= 'constant', solver = 'lbfgs', max_iter= 200) - 0.7777777777777778
model = MLPClassifier(activation = 'logistic', alpha= 0.0001, hidden_layer_sizes= (20, 20), learning_rate= 'invscaling', max_iter= 500, solver= 'lbfgs')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(y_pred)
print(y_test)

from sklearn.metrics import accuracy_score
acc = accuracy_score(y_pred, y_test)
print(acc)

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

rf_model = RandomForestClassifier(
    n_estimators=300,
    max_depth=None,
    n_jobs=-1,
    class_weight="balanced",
    random_state=42,
)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
rf_acc = accuracy_score(y_test, rf_pred)
print("RandomForest accuracy:", rf_acc)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_train)

xgb_model = XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=1.0,
    n_jobs=-1,
    random_state=42,
    tree_method="hist",
    eval_metric="logloss",
)

xgb_model.fit(X_train, y_encoded)
xgb_pred = xgb_model.predict(X_test)
xgb_pred = label_encoder.inverse_transform(xgb_pred)
xgb_acc = accuracy_score(y_test, xgb_pred)
print("XGBoost accuracy:", xgb_acc)

from sklearn.neural_network import MLPClassifier
mlp_model = MLPClassifier(hidden_layer_sizes=(20,20),random_state=42)
mlp_model.fit(X_train, y_train)
y_pred = mlp_model.predict(X_test)
from sklearn.metrics import accuracy_score
acc = accuracy_score(y_pred, y_test)
print(acc)

"""Xgboost Grid Search"""
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder

parameter_space = {
  'max_depth': [3, 4, 5],
  'min_child_weight': [1, 3, 5],
  'subsample': [0.7, 0.8, 0.9],
  'colsample_bytree': [0.7, 0.8, 0.9],
  'gamma': [0, 0.1, 0.3],
  'learning_rate': [0.05, 0.1, 0.2],
  'n_estimators': [100, 200, 300],
  'reg_alpha': [0, 0.1, 1],
  'reg_lambda': [1, 2, 5]
}

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_train)

xgb = XGBClassifier(random_state=42)

#xgb = GridSearchCV(xgb, parameter_space, n_jobs=-1, cv=2)
#xgb.fit(X_train, y_encoded)

#print('Best parameters found:\n', xgb.best_params_)

from math import gamma
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
#replace the values with the output of grid search
xgb_model = XGBClassifier(
    gamma=0.1,
    min_child_weight=5,
    n_estimators=300,
    learning_rate=0.1,
    max_depth=1,
    subsample=0.6,
    colsample_bytree=0.5,
    reg_lambda=1,
    reg_alpha=0.5,
    n_jobs=-1,
    random_state=42,
)

xgb_model.fit(X_train, y_encoded)

#Find accuracy of test set
xgb_pred = xgb_model.predict(X_test)
xgb_pred = label_encoder.inverse_transform(xgb_pred)
xgb_acc = accuracy_score(y_test, xgb_pred)
print("XGBoost test accuracy:", xgb_acc)

#Find accuracy of train set
xgb_pred = xgb_model.predict(X_train)
xgb_pred = label_encoder.inverse_transform(xgb_pred)
xgb_acc = accuracy_score(y_train, xgb_pred)
print("XGBoost train accuracy:", xgb_acc)
#Record the results
#If mismatch adjust and print it again

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Decision Tree Classifier Grid Search
dt = DecisionTreeClassifier(random_state=42)

parameter_space_dt = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

dt_grid = GridSearchCV(dt, parameter_space_dt, n_jobs=-1, cv=2)
dt_grid.fit(X_train, y_train)

print('Best parameters found for Decision Tree Classifier:\n', dt_grid.best_params_)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Decision Tree Classifier Runner
dt_model = DecisionTreeClassifier(criterion='gini', max_depth=10, min_samples_leaf=2, min_samples_split=7, random_state=42)

dt_model.fit(X_train, y_train)

# Find accuracy of test set
dt_pred_test = dt_model.predict(X_test)
dt_acc_test = accuracy_score(y_test, dt_pred_test)
print("Decision Tree Classifier test accuracy:", dt_acc_test)

# Find accuracy of train set
dt_pred_train = dt_model.predict(X_train)
dt_acc_train = accuracy_score(y_train, dt_pred_train)
print("Decision Tree Classifier train accuracy:", dt_acc_train)

"""Random Forest Grid Search"""
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

rf = RandomForestClassifier(random_state=42)

parameter_space = {
    'n_estimators': [100, 200, 500, 700, 1000],
    'max_depth': [None, 10 ,20],
    'min_samples_split': [2],
    'min_samples_leaf': [1],
    'max_features': ['sqrt']
}

rf = GridSearchCV(rf, parameter_space, n_jobs=-1, cv=2)
rf.fit(X_train, y_train)

print('Best parameters found:\n', rf.best_params_)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

rf_model = RandomForestClassifier(
    n_estimators=200,
    max_features="log2",
    max_depth=10,
    min_samples_leaf=4,
    min_samples_split=5,
    random_state=42,
)

rf_model.fit(X_train, y_train)

# Find accuracy of test set
rf_pred_test = rf_model.predict(X_test)
rf_acc_test = accuracy_score(y_test, rf_pred_test)
print("RandomForest test accuracy:", rf_acc_test)

# Find accuracy of train set
rf_pred_train = rf_model.predict(X_train)
rf_acc_train = accuracy_score(y_train, rf_pred_train)
print("RandomForest train accuracy:", rf_acc_train)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

# KNeighbors Grid Search
knn = KNeighborsClassifier()

parameter_space_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

knn_grid = GridSearchCV(knn, parameter_space_knn, n_jobs=-1, cv=2)
knn_grid.fit(X_train, y_train)

print('Best parameters found for KNeighbors Classifier:\n', knn_grid.best_params_)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# KNeighbors Classifier Runner
knn_model = KNeighborsClassifier(metric='euclidean', n_neighbors=5, weights='uniform')

knn_model.fit(X_train, y_train)

# Find accuracy of test set
knn_pred_test = knn_model.predict(X_test)
knn_acc_test = accuracy_score(y_test, knn_pred_test)
print("KNeighbors Classifier test accuracy:", knn_acc_test)

# Find accuracy of train set
knn_pred_train = knn_model.predict(X_train)
knn_acc_train = accuracy_score(y_train, knn_pred_train)
print("KNeighbors Classifier train accuracy:", knn_acc_train)

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV

# MLPClassifier Grid Search
mlp = MLPClassifier(random_state=42)

parameter_space_mlp = {
    'hidden_layer_sizes': [(20,20,5),(24,23),(22,23),(20,20),(10,15),(3,4,5),(5,21,22),(20,25),(21,22),(25,25,20,30),(13,13,15),(3,4,5,6,7,8,9,10,11,12),(3,4),(20),(3,4,5,6),(10),(5)],
    'activation':['identity', 'logistic','tanh','relu'],
    'solver': ['lbfgs','sgd','adam'],
    'alpha': [0.0001,0.05, 0.001, 0.01, 0.5],
    'learning_rate': ['constant','adaptive','invscaling'],
    'max_iter': [100,200,500,1000] # Removed max_iter=1 as it's too small
}

mlp_grid = GridSearchCV(mlp, parameter_space_mlp, n_jobs=-1, cv=2)
mlp_grid.fit(X_train, y_train)

print('Best parameters found for MLPClassifier:\n', mlp_grid.best_params_)

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

mlp_model = MLPClassifier(hidden_layer_sizes=(22,23), activation='logistic', alpha = 0.05, learning_rate= 'invscaling', solver = 'adam', max_iter= 1500, random_state=42)

mlp_model.fit(X_train, y_train)

# Find accuracy of test set
mlp_pred_test = mlp_model.predict(X_test)
mlp_acc_test = accuracy_score(y_test, mlp_pred_test)
print("MLPClassifier test accuracy:", mlp_acc_test)

# Find accuracy of train set
mlp_pred_train = mlp_model.predict(X_train)
mlp_acc_train = accuracy_score(y_train, mlp_pred_train)
print("MLPClassifier train accuracy:", mlp_acc_train)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Logistic Regression Grid Search
lr = LogisticRegression(random_state=42, max_iter=1000)

parameter_space_lr = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}

lr_grid = GridSearchCV(lr, parameter_space_lr, n_jobs=-1, cv=2)
lr_grid.fit(X_train, y_train)

print('Best parameters found for Logistic Regression:\n', lr_grid.best_params_)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Logistic Regression Runner
lr_model = LogisticRegression(C=0.001, penalty='l2', solver='newton-cg', random_state=42, max_iter=4000)

lr_model.fit(X_train, y_train)

# Find accuracy of test set
lr_pred_test = lr_model.predict(X_test)
lr_acc_test = accuracy_score(y_test, lr_pred_test)
print("Logistic Regression test accuracy:", lr_acc_test)

# Find accuracy of train set
lr_pred_train = lr_model.predict(X_train)
lr_acc_train = accuracy_score(y_train, lr_pred_train)
print("Logistic Regression train accuracy:", lr_acc_train)
